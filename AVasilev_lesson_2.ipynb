{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание к уроку 2\n",
    "Исполнитель: Васильев А.\n",
    "\n",
    "Продолжим обработку данных с Твиттера. \n",
    "\n",
    "#### Задания:\n",
    "1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    " * Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    " * Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    " * Исключим стоп-слова с помощью stop_words='english'. \n",
    " * Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n",
    " \n",
    "2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    " * Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    " * Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    " * Исключим стоп-слова с помощью stop_words='english'.\n",
    " * Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n",
    " \n",
    "3. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combine_df.pickle', 'rb') as f:\n",
    "    combine_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>model love you take with you all the time in ur</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet  \\\n",
       "id                                                             \n",
       "1     0.0  when father is dysfunctional and is so selfish...   \n",
       "2     0.0  thanks for lyft credit cannot use cause they d...   \n",
       "3     0.0                                bihday your majesty   \n",
       "4     0.0    model love you take with you all the time in ur   \n",
       "5     0.0                  factsguide society now motivation   \n",
       "\n",
       "                                          tweet_token  \\\n",
       "id                                                      \n",
       "1   [when, father, is, dysfunctional, and, is, so,...   \n",
       "2   [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "3                             [bihday, your, majesty]   \n",
       "4   [model, love, you, take, with, you, all, the, ...   \n",
       "5              [factsguide, society, now, motivation]   \n",
       "\n",
       "                                 tweet_token_filtered  \\\n",
       "id                                                      \n",
       "1   [father, dysfunctional, selfish, drags, kids, ...   \n",
       "2   [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "3                                   [bihday, majesty]   \n",
       "4                       [model, love, take, time, ur]   \n",
       "5                   [factsguide, society, motivation]   \n",
       "\n",
       "                                        tweet_stemmed  \\\n",
       "id                                                      \n",
       "1   [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "2   [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "3                                   [bihday, majesti]   \n",
       "4                       [model, love, take, time, ur]   \n",
       "5                         [factsguid, societi, motiv]   \n",
       "\n",
       "                                     tweet_lemmatized  \n",
       "id                                                     \n",
       "1   [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "2   [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "3                                   [bihday, majesty]  \n",
       "4                       [model, love, take, time, ur]  \n",
       "5                   [factsguide, society, motivation]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = combine_df.set_index('id')\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Использую CountVectorizer для создания мешков слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 'tweet_stemmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vec_st = CountVectorizer(max_df=0.9, max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27721</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48675</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abl  absolut  accept  account  act  action  actor  actual  ad  adapt  \\\n",
       "852      0        0       0        0    0       0      0       0   0      0   \n",
       "49111    0        0       0        0    0       0      0       0   0      0   \n",
       "27721    0        0       0        0    0       0      0       0   0      0   \n",
       "48675    0        0       0        0    0       0      0       0   0      0   \n",
       "13035    0        0       0        0    0       0      0       0   0      0   \n",
       "\n",
       "       ...  yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n",
       "852    ...     0     0          0   0     0     0      0       0   0      0  \n",
       "49111  ...     0     0          0   0     0     0      0       0   0      0  \n",
       "27721  ...     0     0          0   0     0     0      0       0   0      0  \n",
       "48675  ...     0     0          0   0     0     0      0       0   0      0  \n",
       "13035  ...     0     0          0   0     0     0      0       0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tweet_stemmed_c_vec = c_vec_st.fit_transform([' '.join(sent) for sent in combine_df.tweet_stemmed])\n",
    "# Обображаю Bag-of-Words как DataFrame\n",
    "pd.DataFrame(bow_tweet_stemmed_c_vec.toarray(), columns = c_vec_st.get_feature_names()).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 'tweet_lemmatized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vec_lm = CountVectorizer(max_df=0.9, max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adult</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16270</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17735</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14737</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25962</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "16270     0           0        0    0       0      0         0      0    0   \n",
       "17735     0           0        0    0       0      0         0      0    0   \n",
       "2318      0           0        0    0       0      0         0      0    0   \n",
       "14737     0           0        0    0       0      0         0      0    0   \n",
       "25962     0           0        0    0       0      0         0      0    0   \n",
       "\n",
       "       adult  ...  year  yes  yesterday  yo  yoga  york  young  youtube  yr  \\\n",
       "16270      0  ...     0    0          0   0     0     0      0        0   0   \n",
       "17735      0  ...     0    0          0   0     0     0      0        0   0   \n",
       "2318       0  ...     0    0          0   0     0     0      0        0   0   \n",
       "14737      0  ...     0    0          0   0     0     0      0        0   0   \n",
       "25962      0  ...     0    0          0   0     0     0      0        0   0   \n",
       "\n",
       "       yummy  \n",
       "16270      0  \n",
       "17735      0  \n",
       "2318       0  \n",
       "14737      0  \n",
       "25962      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tweet_lemmatized_c_vec = c_vec_lm.fit_transform([' '.join(sent) for sent in combine_df.tweet_lemmatized])\n",
    "# Обображаю Bag-of-Words как DataFrame\n",
    "pd.DataFrame(bow_tweet_lemmatized_c_vec.toarray(), columns = c_vec_lm.get_feature_names()).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Использую CountVectorizer для создания мешков слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 'tweet_stemmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec_st = TfidfVectorizer(max_df=0.9, max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16387</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33177</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10087</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36465</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.569506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abl  absolut  accept  account  act  action  actor    actual   ad  \\\n",
       "16387  0.0      0.0     0.0      0.0  0.0     0.0    0.0  0.000000  0.0   \n",
       "33177  0.0      0.0     0.0      0.0  0.0     0.0    0.0  0.000000  0.0   \n",
       "10087  0.0      0.0     0.0      0.0  0.0     0.0    0.0  0.000000  0.0   \n",
       "19681  0.0      0.0     0.0      0.0  0.0     0.0    0.0  0.000000  0.0   \n",
       "36465  0.0      0.0     0.0      0.0  0.0     0.0    0.0  0.569506  0.0   \n",
       "\n",
       "       adapt  ...  yeah  year  yesterday   yo  yoga  york  young  youtub   yr  \\\n",
       "16387    0.0  ...   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0   \n",
       "33177    0.0  ...   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0   \n",
       "10087    0.0  ...   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0   \n",
       "19681    0.0  ...   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0   \n",
       "36465    0.0  ...   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0   \n",
       "\n",
       "       yummi  \n",
       "16387    0.0  \n",
       "33177    0.0  \n",
       "10087    0.0  \n",
       "19681    0.0  \n",
       "36465    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tweet_stemmed_tf_vec =tf_vec_st.fit_transform([' '.join(sent) for sent in combine_df.tweet_stemmed])\n",
    "# Обображаю Bag-of-Words как DataFrame\n",
    "pd.DataFrame(bow_tweet_stemmed_tf_vec.toarray(), columns = tf_vec_st.get_feature_names()).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 'tweet_lemmatized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec_lm = TfidfVectorizer(max_df=0.9, max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adult</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21684</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46981</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40721</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "21684   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "17103   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "46981   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "21586   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "40721   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "\n",
       "       adult  ...  year  yes  yesterday   yo  yoga  york  young  youtube   yr  \\\n",
       "21684    0.0  ...   0.0  0.0   0.000000  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "17103    0.0  ...   0.0  0.0   0.000000  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "46981    0.0  ...   0.0  0.0   0.000000  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "21586    0.0  ...   0.0  0.0   0.490425  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "40721    0.0  ...   0.0  0.0   0.000000  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "\n",
       "       yummy  \n",
       "21684    0.0  \n",
       "17103    0.0  \n",
       "46981    0.0  \n",
       "21586    0.0  \n",
       "40721    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tweet_lemmatized_tf_vec = tf_vec_lm.fit_transform([' '.join(sent) for sent in combine_df.tweet_lemmatized])\n",
    "# Обображаю Bag-of-Words как DataFrame\n",
    "pd.DataFrame(bow_tweet_lemmatized_tf_vec.toarray(), columns = tf_vec_lm.get_feature_names()).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаю данные\n",
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаю датафрейм\n",
    "corpus_df = pd.DataFrame()\n",
    "corpus_df['text'] = texts\n",
    "corpus_df['label'] = labels\n",
    "corpus_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(corpus_df['text'], corpus_df['label'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применю обученные векторайзеры к данным corpus и посмотрю получившиеся значения accuracy. В качестве классификатора буду использовать логистическую регрессию, т.к. она тратит значительно меньше ресурсов, чем нейронные сети. Для оценки качества векторайзеров в рамках данного практического задания меньшая точность регрессионной модели не должна быть критичной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7376"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer (stemmed)\n",
    "X_train_count_vec_st =  c_vec_st.transform(X_train)\n",
    "X_valid_count_vec_st =  c_vec_st.transform(X_valid)\n",
    "\n",
    "clf_count_st = LogisticRegression(max_iter=200)\n",
    "clf_count_st.fit(X_train_count_vec_st, y_train)\n",
    "preds_count_st = clf_count_st.predict(X_valid_count_vec_st)\n",
    "\n",
    "accuracy_score(y_valid, preds_count_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.552"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer (lemmatized)\n",
    "X_train_count_vec_lm =  c_vec_lm.transform(X_train)\n",
    "X_valid_count_vec_lm =  c_vec_lm.transform(X_valid)\n",
    "\n",
    "clf_count_lm = LogisticRegression(max_iter=200)\n",
    "clf_count_lm.fit(X_train_count_vec_lm, y_train)\n",
    "preds_count_lm = clf_count_st.predict(X_valid_count_vec_lm)\n",
    "\n",
    "accuracy_score(y_valid, preds_count_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7504"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer (stemmed)\n",
    "X_train_tfidf_vec_st = tf_vec_st.transform(X_train)\n",
    "X_valid_tfidf_vec_st = tf_vec_st.transform(X_valid)\n",
    "\n",
    "clf_tfidf_st = LogisticRegression()\n",
    "clf_tfidf_st.fit(X_train_tfidf_vec_st, y_train)\n",
    "preds_tfidf_st = clf_tfidf_st.predict(X_valid_count_vec_st)\n",
    "\n",
    "accuracy_score(y_valid, preds_tfidf_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer (lemmatized)\n",
    "X_train_tfidf_vec_lm = tf_vec_lm.transform(X_train)\n",
    "X_valid_tfidf_vec_lm = tf_vec_lm.transform(X_valid)\n",
    "\n",
    "clf_tfidf_lm = LogisticRegression()\n",
    "clf_tfidf_lm.fit(X_train_tfidf_vec_lm, y_train)\n",
    "preds_tfidf_lm = clf_tfidf_lm.predict(X_valid_count_vec_lm)\n",
    "\n",
    "accuracy_score(y_valid, preds_tfidf_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаю GRID SEARCH параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df_list = [0.8, 0.9, 1.0]\n",
    "max_features_list = [500, 800, 1000, 2000, 3000, 4000, 5000]\n",
    "min_df_list = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# result_list = []\n",
    "# for max_df, max_features, min_df in product(max_df_list, max_features_list, min_df_list):\n",
    "#     c_vec_st_ = CountVectorizer(max_df=max_df, max_features=max_features, \n",
    "#                                 min_df=min_df, stop_words='english')\n",
    "#     c_vec_st_.fit([' '.join(sent) for sent in combine_df.tweet_stemmed])\n",
    "#     X_train_count_vec_st_ =  c_vec_st_.transform(X_train)\n",
    "#     X_valid_count_vec_st_ =  c_vec_st_.transform(X_valid)\n",
    "#     clf_count_st_ = LogisticRegression(max_iter=200)\n",
    "#     clf_count_st_.fit(X_train_count_vec_st_, y_train)\n",
    "#     preds_count_st_ = clf_count_st_.predict(X_valid_count_vec_st_)\n",
    "#     result_list.append({'max_features': max_features,\n",
    "#                         'max_df': max_df, 'min_df': min_df, \n",
    "#                         'vect_type': 'count_stemmed', \n",
    "#                         'accuracy': accuracy_score(y_valid, preds_count_st_)})\n",
    "    \n",
    "#     c_vec_lm_ = CountVectorizer(max_df=max_df, max_features=max_features, \n",
    "#                                 min_df=min_df, stop_words='english')\n",
    "#     c_vec_lm_.fit([' '.join(sent) for sent in combine_df.tweet_lemmatized])\n",
    "#     X_train_count_vec_lm_ =  c_vec_lm_.transform(X_train)\n",
    "#     X_valid_count_vec_lm_ =  c_vec_lm_.transform(X_valid)\n",
    "#     clf_count_lm_ = LogisticRegression(max_iter=200)\n",
    "#     clf_count_lm_.fit(X_train_count_vec_lm_, y_train)\n",
    "#     preds_count_lm_ = clf_count_lm_.predict(X_valid_count_vec_lm_)\n",
    "#     result_list.append({'max_features': max_features,\n",
    "#                         'max_df': max_df, 'min_df': min_df, \n",
    "#                         'vect_type': 'count_lemmatized', \n",
    "#                         'accuracy': accuracy_score(y_valid, preds_count_lm_)})\n",
    "    \n",
    "#     tf_vec_st_ = TfidfVectorizer(max_df=max_df, max_features=max_features, \n",
    "#                                  min_df=min_df, stop_words='english')\n",
    "#     tf_vec_st_.fit([' '.join(sent) for sent in combine_df.tweet_stemmed])\n",
    "#     X_train_tfidf_vec_st_ =  tf_vec_st_.transform(X_train)\n",
    "#     X_valid_tfidf_vec_st_ =  tf_vec_st_.transform(X_valid)\n",
    "#     clf_tfidf_st_ = LogisticRegression(max_iter=200)\n",
    "#     clf_tfidf_st_.fit(X_train_tfidf_vec_st_, y_train)\n",
    "#     preds_tfidf_st_ = clf_tfidf_st_.predict(X_valid_tfidf_vec_st_)\n",
    "#     result_list.append({'max_features': max_features,\n",
    "#                         'max_df': max_df, 'min_df': min_df, \n",
    "#                         'vect_type': 'tfidf_stemmed', \n",
    "#                         'accuracy': accuracy_score(y_valid, preds_tfidf_st_)})\n",
    "    \n",
    "#     tf_vec_lm_ = TfidfVectorizer(max_df=max_df, max_features=max_features, \n",
    "#                                  min_df=min_df, stop_words='english')\n",
    "#     tf_vec_lm_.fit([' '.join(sent) for sent in combine_df.tweet_lemmatized])\n",
    "#     X_train_tfidf_vec_lm_ =  tf_vec_lm_.transform(X_train)\n",
    "#     X_valid_tfidf_vec_lm_ =  tf_vec_lm_.transform(X_valid)\n",
    "#     clf_tfidf_lm_ = LogisticRegression(max_iter=200)\n",
    "#     clf_tfidf_lm_.fit(X_train_tfidf_vec_lm_, y_train)\n",
    "#     preds_tfidf_lm_ = clf_tfidf_lm_.predict(X_valid_tfidf_vec_lm_)\n",
    "#     result_list.append({'max_features': max_features,\n",
    "#                         'max_df': max_df, 'min_df': min_df, \n",
    "#                         'vect_type': 'tfidf_lemmatized', \n",
    "#                         'accuracy': accuracy_score(y_valid, preds_tfidf_lm_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('result_list.json', 'w') as f:\n",
    "#     json.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_list.json', 'r') as f:\n",
    "    result_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>count_stemmed</td>\n",
       "      <td>0.7052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.7308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>tfidf_stemmed</td>\n",
       "      <td>0.7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.7348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>count_stemmed</td>\n",
       "      <td>0.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_features  max_df  min_df         vect_type  accuracy\n",
       "0           500     0.8       1     count_stemmed    0.7052\n",
       "1           500     0.8       1  count_lemmatized    0.7308\n",
       "2           500     0.8       1     tfidf_stemmed    0.7200\n",
       "3           500     0.8       1  tfidf_lemmatized    0.7348\n",
       "4           500     0.8       2     count_stemmed    0.7052"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_features                5000\n",
       "max_df                       0.8\n",
       "min_df                         2\n",
       "vect_type       count_lemmatized\n",
       "accuracy                  0.8372\n",
       "Name: 77, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.iloc[np.argmax(result_df.accuracy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_features  max_df  min_df         vect_type  accuracy\n",
       "73           5000     0.8       1  count_lemmatized    0.8360\n",
       "75           5000     0.8       1  tfidf_lemmatized    0.8348\n",
       "157          5000     0.9       1  count_lemmatized    0.8360\n",
       "159          5000     0.9       1  tfidf_lemmatized    0.8348\n",
       "241          5000     1.0       1  count_lemmatized    0.8360\n",
       "243          5000     1.0       1  tfidf_lemmatized    0.8348\n",
       "77           5000     0.8       2  count_lemmatized    0.8372\n",
       "79           5000     0.8       2  tfidf_lemmatized    0.8372\n",
       "161          5000     0.9       2  count_lemmatized    0.8372\n",
       "163          5000     0.9       2  tfidf_lemmatized    0.8372\n",
       "245          5000     1.0       2  count_lemmatized    0.8372\n",
       "247          5000     1.0       2  tfidf_lemmatized    0.8372\n",
       "83           5000     0.8       3  tfidf_lemmatized    0.8324\n",
       "167          5000     0.9       3  tfidf_lemmatized    0.8324\n",
       "251          5000     1.0       3  tfidf_lemmatized    0.8324"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[result_df.accuracy > 0.83, :].sort_values('min_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>count_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>0.8372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_features  max_df  min_df         vect_type  accuracy\n",
       "77           5000     0.8       2  count_lemmatized    0.8372\n",
       "79           5000     0.8       2  tfidf_lemmatized    0.8372\n",
       "161          5000     0.9       2  count_lemmatized    0.8372\n",
       "163          5000     0.9       2  tfidf_lemmatized    0.8372\n",
       "245          5000     1.0       2  count_lemmatized    0.8372\n",
       "247          5000     1.0       2  tfidf_lemmatized    0.8372"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[result_df.accuracy == 0.8372, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось 6 наборов параметров, давших одинаковое значение метрики. Очевидно, что основным параметром, дающим прирост значения метрики является max_features. Также векторайзеры, обученные на лематезированных данных показывают результат лучше, чем векторайзеры, обученные на данных после стеммировани."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8332"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vec_lm = TfidfVectorizer(max_df=0.8, max_features=5000, \n",
    "                             min_df=2, stop_words='english')\n",
    "tf_vec_lm.fit([' '.join(sent) for sent in combine_df.tweet_lemmatized])\n",
    "X_train_tfidf_vec_lm =  tf_vec_lm.transform(X_train)\n",
    "X_valid_tfidf_vec_lm =  tf_vec_lm.transform(X_valid)\n",
    "clf_tfidf_lm = LogisticRegression(max_iter=200)\n",
    "clf_tfidf_lm.fit(X_train_tfidf_vec_lm, y_train)\n",
    "preds_tfidf_lm = clf_tfidf_lm.predict(X_valid_tfidf_vec_lm)\n",
    "accuracy_score(y_valid, preds_tfidf_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применю PCA для того, чтобы уменьшить размерность данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ = PCA(n_components=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before PCA: (7500, 5000), after PCA: (7500, 500)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf_vec_lm_pca = pca_.fit_transform(X_train_tfidf_vec_lm.toarray())\n",
    "X_valid_tfidf_vec_lm_pca = pca_.transform(X_valid_tfidf_vec_lm.toarray())\n",
    "print(f'Before PCA: {X_train_tfidf_vec_lm.shape}, after PCA: {X_train_tfidf_vec_lm_pca.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8212"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tfidf_lm_pca = LogisticRegression(max_iter=200)\n",
    "clf_tfidf_lm_pca.fit(X_train_tfidf_vec_lm_pca, y_train)\n",
    "preds_tfidf_lm_pca = clf_tfidf_lm_pca.predict(X_valid_tfidf_vec_lm_pca)\n",
    "accuracy_score(y_valid, preds_tfidf_lm_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components_list = [100, 300, 500, 800, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pca_results = []\n",
    "# for n_components in n_components_list:\n",
    "#     pca_ = PCA(n_components=n_components)\n",
    "#     X_train_tfidf_vec_lm_pca_ = pca_.fit_transform(X_train_tfidf_vec_lm.toarray())\n",
    "#     X_valid_tfidf_vec_lm_pca_ = pca_.transform(X_valid_tfidf_vec_lm.toarray())\n",
    "#     clf_tfidf_lm_pca_ = LogisticRegression(max_iter=200)\n",
    "#     clf_tfidf_lm_pca_.fit(X_train_tfidf_vec_lm_pca_, y_train)\n",
    "#     preds_tfidf_lm_pca_ = clf_tfidf_lm_pca_.predict(X_valid_tfidf_vec_lm_pca_)\n",
    "#     pca_results.append({'n_components': n_components,\n",
    "#                        'accuracy': accuracy_score(y_valid, preds_tfidf_lm_pca_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pca_results.json', 'w') as f:\n",
    "#     json.dump(pca_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pca_results.json', 'r') as f:\n",
    "    pca_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_components</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.8004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>0.8284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>0.8228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800</td>\n",
       "      <td>0.8264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.8292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_components  accuracy\n",
       "0           100    0.8004\n",
       "1           300    0.8284\n",
       "2           500    0.8228\n",
       "3           800    0.8264\n",
       "4          1000    0.8292"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_df = pd.DataFrame(pca_results)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из полученных результатов, оптимальным количеством компонент является 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train before PCA: (7500, 5000), after PCA: (7500, 300)\n",
      "Valid before PCA: (2500, 5000), after PCA: (2500, 300)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=300)\n",
    "X_train_pca = pca.fit_transform(X_train_tfidf_vec_lm.toarray())\n",
    "X_valid_pca = pca.transform(X_valid_tfidf_vec_lm.toarray())\n",
    "print(f'Train before PCA: {X_train_tfidf_vec_lm.shape}, after PCA: {X_train_pca.shape}')\n",
    "print(f'Valid before PCA: {X_valid_tfidf_vec_lm.shape}, after PCA: {X_valid_pca.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8176"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pca = LogisticRegression(max_iter=200)\n",
    "clf_pca.fit(X_train_pca, y_train)\n",
    "preds_pca = clf_pca.predict(X_valid_pca)\n",
    "accuracy_score(y_valid, preds_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После приминения PCA и уменьшения количества признаков с 5000 до 300 значение accuracy уменьшилось на 1.87%. Считаю, что такая оперя в качестве модели приемлема, учитывая уменьшение размерности датафрейма на порядок, а следовательно значительное сокращение ресурсозатрат при работе модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
